{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c90da2",
   "metadata": {},
   "source": [
    "# ðŸ“˜ LangChain Study Reference Notebook\n",
    "This notebook provides a structured reference for working with **LangChain** (with Ollama as backend LLM).\n",
    "Each section includes:\n",
    "- Concept explanation\n",
    "- Example code\n",
    "- Notes / references\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26c7db",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Before working with LangChain, ensure you have the required packages installed.\n",
    "Weâ€™ll also define a small helper function to interact with Ollama locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bb57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def get_completion_local(prompt, model=\"llama3\"):\n",
    "    \"\"\"Send a prompt to a local Ollama server and return the response.\"\"\"\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db96db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the translation:\n",
      "\n",
      "\"Wow, I'm really upset that my blender lid came loose and covered my kitchen walls in a mess!\"\n",
      "\n",
      "I've kept the tone calm and respectful as per your request. Let me know if you need any further assistance!\n"
     ]
    }
   ],
   "source": [
    "chat = get_completion_local(\n",
    "\"\"\"Translate the text below into American English\n",
    "in a calm and respectful tone.\n",
    "\n",
    "Text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls!```\"\"\",\n",
    "model=\"llama3\"\n",
    ")\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b66418",
   "metadata": {},
   "source": [
    "## 2. LangChain with Ollama\n",
    "LangChain integrates with Ollama via the `ChatOllama` class. This allows you to invoke models hosted locally through Ollama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f0c2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a language model, I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! It's great to chat with you. How can I help you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0.1)\n",
    "llm.invoke(\"Hello, how are you?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef6729",
   "metadata": {},
   "source": [
    "## 3. Prompts\n",
    "LangChain provides `ChatPromptTemplate` to structure prompts. You can define placeholders and fill them dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f68384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Translate this into French: Good morning!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"Translate this into French: {text}\")\n",
    "prompt = template.format_messages(text=\"Good morning!\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37abff",
   "metadata": {},
   "source": [
    "## 4. Output Parsing\n",
    "Use `StructuredOutputParser` to enforce structured responses from the model. This ensures reliable parsing of outputs into JSON-like structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568aeea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: ```json\n",
      "{\n",
      "  \"answer\": \"LangChain is a large language model that uses a combination of transformer-based architectures and chain-like structures to generate coherent and context-aware text.\",\n",
      "  \"confidence\": \"0.9\"\n",
      "}\n",
      "```\n",
      "\n",
      "Note: The confidence score is an estimate based on my training data, and it may not reflect the actual accuracy or reliability of the answer.\n",
      "Parsed: {'answer': 'LangChain is a large language model that uses a combination of transformer-based architectures and chain-like structures to generate coherent and context-aware text.', 'confidence': '0.9'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"The direct answer\"),\n",
    "    ResponseSchema(name=\"confidence\", description=\"Confidence score between 0 and 1\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question: {question}\\n{format_instructions}\"\n",
    ").partial(format_instructions=format_instructions)\n",
    "\n",
    "messages = prompt.format_messages(question=\"What is LangChain?\")\n",
    "raw_output = llm.invoke(messages)\n",
    "structured = parser.parse(raw_output.content)\n",
    "\n",
    "print(\"Raw:\", raw_output.content)\n",
    "print(\"Parsed:\", structured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb66e0",
   "metadata": {},
   "source": [
    "## 5. Memory\n",
    "LangChain 0.3.x uses **message history** and `RunnableWithMessageHistory`.\n",
    "- `MessagesPlaceholder(\"history\")` defines where past messages appear in the prompt.\n",
    "- `InMemoryChatMessageHistory` stores previous exchanges.\n",
    "- `RunnableWithMessageHistory` automatically injects and updates conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d9bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to! I've taken note that your favorite color is indeed blue. If you need any reminders or have questions, feel free to ask!\n",
      "Easy one! Your favorite color is... BLUE!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "parser = StrOutputParser()\n",
    "base_chain = prompt | model | parser\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chat = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"demo\"}}\n",
    "\n",
    "print(chat.invoke({\"input\": \"Hi, please remember my favorite color is blue.\"}, config=config))\n",
    "print(chat.invoke({\"input\": \"What is my favorite color?\"}, config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740a4d9",
   "metadata": {},
   "source": [
    "## 6. Adding Messages Manually\n",
    "You can inject messages into history directly:\n",
    "- `add_user_message(\"...\")`\n",
    "- `add_ai_message(\"...\")`\n",
    "\n",
    "### Advantages:\n",
    "- Preload user profile or facts\n",
    "- Restore state across sessions\n",
    "- Simulate interactions in testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68fe423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage : Hi, please remember my favorite color is blue.\n",
      "AIMessage : I'd be happy to! I've taken note that your favorite color is indeed blue. If you need any reminders or have questions, feel free to ask!\n",
      "HumanMessage : What is my favorite color?\n",
      "AIMessage : Easy one! Your favorite color is... BLUE!\n",
      "HumanMessage : Remind me about LangChain.\n",
      "AIMessage : LangChain is a framework for building with LLMs.\n"
     ]
    }
   ],
   "source": [
    "history = get_session_history(\"demo\")\n",
    "history.add_user_message(\"Remind me about LangChain.\")\n",
    "history.add_ai_message(\"LangChain is a framework for building with LLMs.\")\n",
    "\n",
    "for m in history.messages:\n",
    "    print(type(m).__name__, \":\", m.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c70fde",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Ollama Documentation](https://ollama.com/)\n",
    "- [LangChain Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "- [Memory in LangChain](https://python.langchain.com/docs/expression_language/how_to/message_history/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
